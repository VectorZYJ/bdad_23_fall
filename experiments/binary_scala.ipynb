{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "import org.apache.spark.sql.functions._\n",
        "import org.apache.spark.ml.Pipeline\n",
        "import org.apache.spark.ml.classification.LogisticRegression\n",
        "import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer}\n",
        "import org.apache.spark.ml.Pipeline\n",
        "import org.apache.spark.sql.DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## data loading and preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "val filePath = \"buysell.csv\"\n",
        "val newFilePath = \"new_data.csv\"\n",
        "val originalDF = spark.read\n",
        "   .option(\"header\", \"true\")\n",
        "   .option(\"inferSchema\", \"true\")\n",
        "  .csv(newFilePath)\n",
        "  .withColumnRenamed(\"BuySell\",\"buySell\")\n",
        "  .withColumn(\"buySell\",col(\"buySell\").cast(\"double\"))\n",
        "\n",
        "z.show(originalDF)\n",
        "// val TechDF = rawDF.filter(col(\"Category\")===\"Tech\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "rawDF.printSchema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "import org.apache.spark.sql.expressions.Window\n",
        "import org.apache.spark.sql.functions.{col, lead, lag}\n",
        "\n",
        "problemDF = rawDF.withColumn(\"difference\",)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "originalDF.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "val rawDF = originalDF.select(\"Date\", \"Category\", \"avg_sentiment\", \"avg_subjectivity\", \"avg_articles_count\", \"buySell\")\n",
        "z.show(rawDF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "// split the dataset\n",
        "val sortedDF = rawDF.orderBy(asc(\"Date\")) // replace the data with the proper name\n",
        "\n",
        "val splitPoint = (sortedDF.count * 0.8).toInt  \n",
        "val trainDF = sortedDF.limit(splitPoint)\n",
        "val testDF = sortedDF.except(trainDF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### linear regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "import org.apache.spark.ml.feature.RFormula\n",
        "// val rFormula = new RFormula()\n",
        "//   .setFormula(\"sectorIndex ~ . - Date - buySell\")\n",
        "//   .setFeaturesCol(\"features\")\n",
        "//   .setLabelCol(\"sectorIndex\")\n",
        "//   .setHandleInvalid(\"skip\")\n",
        "   \n",
        "// import org.apache.spark.ml.regression.LinearRegression\n",
        "\n",
        "// val linearRegression = new LinearRegression() \n",
        "//   .setFeaturesCol(\"features\")\n",
        "//   .setLabelCol(\"sectorIndex\")\n",
        "  \n",
        "// val pipeline = new Pipeline().setStages(Array(rFormula, linearRegression))\n",
        "\n",
        "\n",
        "// buysell as the label\n",
        "import org.apache.spark.ml.feature.RFormula\n",
        "val rFormula = new RFormula()\n",
        "   .setFormula(\"buysell ~ . - Date - sectorIndex\")\n",
        "   .setFeaturesCol(\"features\")\n",
        "   .setLabelCol(\"buysell\")\n",
        "   .setHandleInvalid(\"skip\")\n",
        "   \n",
        "import org.apache.spark.ml.regression.LinearRegression\n",
        "\n",
        "val linearRegression = new LinearRegression() \n",
        "  .setFeaturesCol(\"features\")\n",
        "  .setLabelCol(\"buysell\")\n",
        "val pipeline = new Pipeline().setStages(Array(rFormula, linearRegression))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "val linearPipelineModel = pipeline.fit(trainDF)\n",
        "val predDF = linearPipelineModel.transform(testDF)\n",
        "\n",
        "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
        "val r2Evaluator = new RegressionEvaluator()\n",
        "   .setPredictionCol(\"prediction\")\n",
        "   .setLabelCol(\"sectorIndex\")\n",
        "   .setMetricName(\"r2\")\n",
        "val r2 = r2Evaluator.evaluate(predDF)\n",
        "println(f\"r2 is $r2%.4f\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "val categories = testDF.select(\"Category\").distinct().collect().map(_(0).toString)\n",
        "\n",
        "\n",
        "categories.foreach { category =>\n",
        "  val filteredDF = testDF.filter(testDF(\"Category\") === category)\n",
        "  val predictions = linearPipelineModel.transform(filteredDF)\n",
        "  \n",
        "  val r2 = r2Evaluator.evaluate(predictions)\n",
        "  println(s\"Category: $category, R2 score: $r2\")\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### logistics regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "rawDF.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%spark\n",
        "import org.apache.spark.ml.feature.RFormula\n",
        "val rFormula = new RFormula()\n",
        "  .setFormula(\"buySell ~ . - Date \")\n",
        "  .setFeaturesCol(\"features\")\n",
        "  .setLabelCol(\"buySell\")\n",
        "  .setHandleInvalid(\"skip\")\n",
        "\n",
        "// // Assemble your features into a single vector column\n",
        "// val assembler = new VectorAssembler()\n",
        "//   .setInputCols(Array(\"Category\", \"avg_sentiment\", \"avg_subjectivity\", \"avg_articles_count\"))\n",
        "//   .setOutputCol(\"features\")\n",
        "\n",
        "// Define the logistic regression model\n",
        "val logisticRegression = new LogisticRegression()\n",
        "  .setLabelCol(\"buySell\")\n",
        "  .setFeaturesCol(\"features\")\n",
        "  .setMaxIter(10)\n",
        "  .setRegParam(0.3)\n",
        "  .setElasticNetParam(0.8)\n",
        "\n",
        "// Set up the pipeline with the stages\n",
        "val logisticPipeline = new Pipeline()\n",
        "  .setStages(Array(rFormula, logisticRegression))\n",
        "\n",
        "// Fit the model\n",
        "val logisticPipelineModel = logisticPipeline.fit(trainDF)\n",
        "\n",
        "// Make predictions\n",
        "val logisticPredDF = logisticPipelineModel.transform(testDF)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "// This will filter out rows where 'buySell' or 'rawPrediction' is null\n",
        "val cleanedDF = logisticPredDF.filter(col(\"buySell\").isNotNull && col(\"rawPrediction\").isNotNull)\n",
        "\n",
        "\n",
        "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
        "\n",
        "val evaluator = new BinaryClassificationEvaluator()\n",
        "  .setLabelCol(\"buySell\")\n",
        "  .setRawPredictionCol(\"rawPrediction\")\n",
        "  .setMetricName(\"areaUnderROC\")\n",
        "\n",
        "val accuracy = evaluator.evaluate(cleanedDF)\n",
        "// val accuracy = evaluator.evaluate(logisticPredDF.withColumn(\"prediction\",$\"prediction\".cast(\"boolean\")))\n",
        "println(s\"Area Under ROC for logstics regression = $accuracy\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "z.show(logisticPredDF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \n",
        "\n",
        "### random forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "import org.apache.spark.ml.feature.{OneHotEncoder, StringIndexer, VectorAssembler}\n",
        "import org.apache.spark.ml.classification.RandomForestClassifier\n",
        "import org.apache.spark.ml.{Pipeline, PipelineStage}\n",
        "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
        "import org.apache.spark.sql.SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "// Define the Random Forest Classifie\n",
        "val rf = new RandomForestClassifier()\n",
        "  .setLabelCol(\"buySell\")\n",
        "  .setFeaturesCol(\"features\")\n",
        "\n",
        "// Create the pipeline with the stages\n",
        "val pipeline = new Pipeline()\n",
        "  .setStages(Array(rFormula, rf))\n",
        "\n",
        "// Train the model\n",
        "val model = pipeline.fit(trainDF)\n",
        "\n",
        "// Make predictions\n",
        "val rfPredDF = model.transform(testDF)\n",
        "\n",
        "val cleanedDF = rfPredDF.filter(col(\"buySell\").isNotNull && col(\"rawPrediction\").isNotNull)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "// Evaluate the model\n",
        "val evaluator = new BinaryClassificationEvaluator()\n",
        "  .setLabelCol(\"buySell\")\n",
        "  .setRawPredictionCol(\"rawPrediction\")\n",
        "  .setMetricName(\"areaUnderROC\")\n",
        "\n",
        "val accuracy = evaluator.evaluate(cleanedDF)\n",
        "println(s\"Area Under ROC = $accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \n",
        "### polynomiaal regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "import org.apache.spark.ml.feature.PolynomialExpansion\n",
        "\n",
        "val polyExpansion = new PolynomialExpansion()\n",
        "  .setInputCol(\"features\") // original features\n",
        "  .setOutputCol(\"polyFeatures\") // polynomial expansion of features\n",
        "  .setDegree(2) // for quadratic terms\n",
        "  \n",
        "  \n",
        "val logisticRegression = new LogisticRegression()\n",
        "  .setLabelCol(\"buySell\")\n",
        "  .setFeaturesCol(\"polyFeatures\")\n",
        "  .setMaxIter(10)\n",
        "  .setRegParam(0.3)\n",
        "  .setElasticNetParam(0.8)\n",
        "\n",
        "// Set up the pipeline with the stages\n",
        "\n",
        "val polyPipeline = new Pipeline()\n",
        "  .setStages(Array(rFormula,polyExpansion, logisticRegression))\n",
        "\n",
        "// Fit the model\n",
        "val polyPipelineModel = logisticPipeline.fit(trainDF)\n",
        "\n",
        "// Make predictions\n",
        "val polyPredDF = polyPipelineModel.transform(testDF)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "// This will filter out rows where 'buySell' or 'rawPrediction' is null\n",
        "val cleanedDF = polyPredDF.filter(col(\"buySell\").isNotNull && col(\"rawPrediction\").isNotNull)\n",
        "\n",
        "\n",
        "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
        "\n",
        "val evaluator = new BinaryClassificationEvaluator()\n",
        "  .setLabelCol(\"buySell\")\n",
        "  .setRawPredictionCol(\"rawPrediction\")\n",
        "  .setMetricName(\"areaUnderROC\")\n",
        "\n",
        "val accuracy = evaluator.evaluate(cleanedDF)\n",
        "// val accuracy = evaluator.evaluate(logisticPredDF.withColumn(\"prediction\",$\"prediction\".cast(\"boolean\")))\n",
        "println(s\"Area Under ROC for polynomial regression = $accuracy\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "spylon-kernel",
      "language": "scala",
      "name": "spylon-kernel"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala",
      "version": "3.10.9"
    },
    "name": "BDAD_project"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
